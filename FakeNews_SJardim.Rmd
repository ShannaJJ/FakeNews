---
output:
  pdf_document: default
  html_document: default
---

---
title: 'Data Science: Capstone - Fake New Detection Project'
author: "Shanna Jardim"
date: "01/02/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## INTRODUCTION

> With all that is going on in the world, fake news, conspiracy theories, over exaggerating the truth have increased.  It is hard to trust mainstream media, social media, any articles. I decided to look into how I can create a Fake New Detection Algorithm.

**Fake new on the Rise** 
> Misleading content such as fake news and fake reviews, have increasingly become dangerous for online users. Social Media is one of the biggest sources of spreading fake new and 
poorly written news articles, which have a certain degree of real news but are not entirely accurate.
Usually, this kind of news is designed to promote a certain agenda or biased opinion.
One reason for the increase in misleading information is how easy it is for anyone to write fake reviews or write fake news on the web and there is no pre-approval process of the writings and spreading false information or beliefs across all social media. This poses a challenge as there is no real way to tell if the content is fake or true. This has given way to the development of Fact checking websites that use machine learning & human research to detect fake news.


## GOAL OF PROJECT

> In this assignment, I will use a dataset from Kaggle that has already split the articles into Fake or True and train an algorithm to detect if a new article is false or fact by simply using the text of that article. 

## DATA SET

I am using the following datasets which have already been split into Fake and True News. 
https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset

This dataset does not have a lot of columns I am therefore going to do a very simple analysis using only the title and article text to identify whether an article is fake or true adding factors discussed in the document.
**Bias's to consider**
Number of False articles is already higher than True.
The source of articles is unknown.
The author is unknown

```{r givencode , include=FALSE}
#Load packages & import data
library(tidyverse) 
library(dplyr)
library(tidytext)
library(tibble)
library(ggplot2)
library(scales)
library(syuzhet)
library(tm)
library(ggplot2)

FakeNews <- read_csv("Fake.csv")
TrueNews <- read_csv("True.csv")

```

## EXPLORING THE DATA

```{r }
head(FakeNews)
head(TrueNews)
```

* **Observations**
+ title and text can be joined into one Text field 
+ Will change the format of Date to year and month
+ Fake or True flag must be added when combining data

```{r}
FakeNews %>% group_by(subject) %>%   summarize(no_articles = n()) 
TrueNews %>% group_by(subject) %>%   summarize(no_articles = n())
```
Subjects that appear in True are not in fake new dataset I feel this adds little meaning to the prediction
 
 
**Word Analysis**

Text Preprocessing - Finding the most common words in fake & true
```{r , include=FALSE}
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("syuzhet")
library("ggplot2")

```

**Fake News Words**
```{r}
# Add title & text to same field
FakeNews$text <- with(FakeNews, paste(title, text))
TrueNews$text <- with(TrueNews, paste(title, text))
# Load the data as a corpus

fake_corpus = VCorpus(VectorSource(FakeNews$text))
# Convert the text to lower case
fake_corpus = tm_map(fake_corpus, content_transformer(tolower))
# Remove numbers
fake_corpus = tm_map(fake_corpus, removeNumbers)
# Remove punctuation
fake_corpus = tm_map(fake_corpus, removePunctuation)
# Remove english common stopwords
fake_corpus = tm_map(fake_corpus, removeWords, stopwords())
# Eliminate extra white spaces
fake_corpus = tm_map(fake_corpus, stripWhitespace)
# Text stemming - which reduces words to their root form. 
# In my research I have noticed that this is not always the most accurate process 
fake_corpus = tm_map(fake_corpus, stemDocument)


fake_dtm = DocumentTermMatrix(fake_corpus)
fake_dtm = removeSparseTerms(fake_dtm, 0.999)
fake_dataset = as.data.frame(as.matrix(fake_dtm))

#wordCloud
library(wordcloud)

fake_v = sort(colSums(fake_dataset),decreasing=TRUE)
myNames = names(fake_v)
fake_words = data.frame(word=myNames,freq=fake_v,type='fake')

wordcloud(words = fake_words$word, freq = fake_words$freq, min.freq = 5,
          max.words=100, random.order=FALSE, rot.per=0.40, 
          colors=brewer.pal(8, "Dark2"))

```

**True News Words**
```{r}
true_corpus = VCorpus(VectorSource(TrueNews$text))
# Convert the text to lower case
true_corpus = tm_map(true_corpus, content_transformer(tolower))
# Remove numbers
true_corpus = tm_map(true_corpus, removeNumbers)
# Remove punctuation
true_corpus = tm_map(true_corpus, removePunctuation)
# Remove Stop words - They are the most commonly occurring words in a language and 
# have very little value in terms of gaining useful information. 
# They should be removed before performing further analysis.
true_corpus = tm_map(true_corpus, removeWords, stopwords())
# Eliminate extra white spaces
true_corpus = tm_map(true_corpus, stripWhitespace)
# Text stemming - which reduces words to their root form. 
# In my research I have noticed that this is not always the most accurate process 
true_corpus = tm_map(true_corpus, stemDocument)

true_dtm = DocumentTermMatrix(true_corpus)
true_dtm = removeSparseTerms(true_dtm, 0.999)
true_dataset = as.data.frame(as.matrix(true_dtm))

#wordCloud
library(wordcloud)

true_v = sort(colSums(true_dataset),decreasing=TRUE)
myNames_true = names(true_v)
true_words = data.frame(word=myNames_true,freq=true_v,type='true')

wordcloud(words = true_words$word, freq = fake_words$freq, min.freq = 5,
          max.words=100, random.order=FALSE, rot.per=0.40, 
          colors=brewer.pal(8, "Dark2"))

Words <- rbind(fake_words, true_words)

```

```{r , echo=FALSE}
# Display the top 5 most frequent words
head(fake_words, 5)
head(true_words, 5)
```
I am going to use the top 5 words in the analysis.
There is a lot more analysis that can be done on the text such as - Word Associations, Term Frequencies etc.
But due to time constraints I will move on. 

## TRANSFORMATION OF THE DATA

A fake or truth indicator must be added as this is what we are predicting.
```{r }
FakeNews$Type <- c('Fake')
TrueNews$Type <- c('True')
```
Join Fake & True Data to make a full data set.
```{r}
NewsData <- rbind(FakeNews, TrueNews)
``` 

Check for any Null Values
```{r}
anyNA(NewsData)

#True Check which columns contain Null
colnames(NewsData)[colSums(is.na(NewsData)) > 0]

# Considering the text column is the main column we are using for analysis & prediction we will remove the NA values 
NewsData <- na.omit(NewsData)

nrow(NewsData)

```

Addition of extra information for predictions

```{r}
# Add an ID 
NewsData$ID <-seq_len(nrow(NewsData))
NewsData <- select(NewsData, ID, everything())

# Remove Title, Date, Subject field
NewsData <- subset(NewsData, select = -c(title,date,subject) )

#Add number of sentences per article 
library(quanteda)
NewsData$No_of_sentences <- nsentence(NewsData$text)
```

```{r}
#Add Number of character per article
NewsData$TextLength <- nchar(NewsData$text)
summary(NewsData$TextLength)
TextLength <- NewsData$TextLength
TextLength[1:50]

ggplot(NewsData, aes(x = TextLength, fill = Type)) +
  theme_bw() +
  geom_histogram(binwidth = 5) +
  labs(y = "Text Count", x = "Length of Text",
       title = "Distribution of Text Lengths by Type")
```
Text data contains characters, like punctuations, stop words etc, 
that does not give information and increase the complexity of the analysis. 
So, in order to simplify our data, we remove all this noise to obtain a clean and analyzable dataset.
```{r}
# Sapply function to calculate number of exclamation marks
NewsData$excl <- sapply(NewsData$text, function(x) length(unlist(strsplit(as.character(x), "\\!+"))))

# Sapply function to calculate number of question marks
NewsData$question <- sapply(NewsData$text, function(x) length(unlist(strsplit(as.character(x), "\\?+"))))

##Count of exclamations & question marks in fake and true news avg in Fake and True
NewsData %>% group_by(Type) %>% summarise(Sum_Excl=sum(excl),
                                          Avg_Excl=mean(excl),
                                          Sum_Ques=sum(question),
                                          Avg_Ques=mean(question))

# Make text lower case
NewsData$text <- tolower(NewsData$text) #make it lower case
NewsData$text<- gsub('[[:punct:]]', '', NewsData$text) #remove punctuation
```

**Removing Stop words from total Dataset**
Remove Stop words - They are the most commonly occurring words in a language and 
have very little value in terms of gaining useful information. 
They should be removed before performing further analysis.
```{r }

```

```{r }
#Add number of words per article after removing Stop words
NewsData$No_of_words <- sapply(strsplit(NewsData$text, " "), length)
```

**Sentiment Analysis**
Sentiment Analysis:The study of extracted information to identify reactions, attitudes, context and emotions.
R offers the get_nrc_sentiment function via the Tidy or Syuzhet packages for analysis of emotion words expressed in text. Both packages implemented Saif Mohammad's NRC Emotion lexicon, comprised of several words for emotion expressions of anger, fear, anticipation, trust, surprise, sadness, joy, and disgust
```{r }
emotion <-get_nrc_sentiment(as.character(NewsData$text))
```
The last 2 columns show Negative & Postive words. Will add this to the dataset.

```{r }
#Take only ID and Fake Column and combine with emotion
IDFAke <- NewsData[c(1,3)]
#Take only the emotions - negative & postive will the used in actual News dataset
emotionDF <- cbind(NewsData[c(3)],emotion[c(1,2,3,4,5,6,7,8)])

emotionGraph <- emotionDF %>% group_by(Type) %>%
  summarize_all(mean)

emotionGraph <- emotionGraph %>% gather("emotion", "Avg_No_of_Words", -Type)
```

```{r }
#Create graph of Emotions Fake vs True Words
ggplot(emotionGraph, aes(x = emotion, y = Avg_No_of_Words, fill=Type)) +
  geom_col(position = "dodge")
```
Interesting Observation is that Trust is the only sentiment on average that is more in true news than in fake news.
I will add this observation to the dataset

```{r }
#taking only negative , positive and trust column for the analysis
emotionNegPos<-emotion[c(8,9,10)]
emotionNegPos$ID <-seq_len(nrow(emotion))
emotionNegPos <- select(emotionNegPos, ID, everything())

NewsData<-left_join(NewsData,emotionNegPos)
```

```{r }
Dataset <- NewsData
```

## MODEL BUILDING & TRAINING

Now that we have prepared the data we can start building and training the data

```{r}
# Check again for any NA data
anyNA(NewsData)
NewsData <- subset(Dataset, select = -c(ID,text))
```

```{r}
#Encoding categorical data
NewsData$Type = factor(NewsData$Type,
                           levels= c('Fake','True'),
                           labels= c(1,0))

# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
library(caret)
set.seed(123)

split = sample.split(NewsData$Type, SplitRatio = 0.8)
train_set = subset(NewsData, split == TRUE)
test_set = subset(NewsData, split == FALSE)
# Feature Scaling
train_set[,2:9] = scale(train_set[,2:9])
test_set[,2:9] = scale(test_set[,2:9])

```

**MODEL 1 - Logistic Regression**
*"Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression)"* ~ WIKIPEDIA

For the first model, I am going to use number of Sentences and Text Length only for prediction

```{r}
# Fitting Logistic Regression to the Training set
classifier = glm(formula = Type ~ No_of_sentences + TextLength,
                 family = binomial,
                 data = train_set)
classifier
summary(classifier)

# Predicting the Test set results
prob_pred = predict(classifier, type = 'response', newdata = test_set[2:3])
y_pred = ifelse(prob_pred > 0.5, 1, 0)
y_pred <-as.factor(y_pred)

# Making the Confusion Matrix
require(caret)    
cm<-confusionMatrix(data=y_pred, 
                    reference=test_set$Type)
cm
Accuracy<-round(cm$overall[1],2)
Accuracy

```
An Accuracy of 51% is quite good considering the simplicity of the measures I am using for prediction

**MODEL 2 - Logistic Regression 2  **

Also using Logistic Regression but now the sentiment of the words. 
```{r}
# Fitting Logistic Regression to the Training set
classifier2 = glm(formula = Type ~ trust + negative + positive,
                 family = binomial,
                 data = train_set)
classifier2
summary(classifier2)

# Predicting the Test set results
prob_pred2 = predict(classifier2, type = 'response', newdata = test_set[7:9])
y_pred2 = ifelse(prob_pred2 > 0.5, 1, 0)
y_pred2 <-as.factor(y_pred2)

# Making the Confusion Matrix
require(caret)    
cm<-confusionMatrix(data=y_pred2, 
                    reference=test_set$Type)
cm
Accuracy<-round(cm$overall[1],2)
Accuracy
```
Less than previous

**MODEL 3 - Logistic Regression 3  **
Using only negative & postive
```{r}
# Fitting Logistic Regression to the Training set
classifier3 = glm(formula = Type ~ negative + positive,
                  family = binomial,
                  data = train_set)
classifier3
summary(classifier3)

# Predicting the Test set results
prob_pred3 = predict(classifier3, type = 'response', newdata = test_set[8:9])

y_pred3 = ifelse(prob_pred3 > 0.5, 1, 0)
y_pred3 <-as.factor(y_pred3)

# Making the Confusion Matrix
require(caret)    
cm<-confusionMatrix(data=y_pred3, 
                    reference=test_set$Type)
cm
Accuracy<-round(cm$overall[1],2)
Accuracy
```

**MODEL 4 - SUPPORT VECTOR MACHINE**
*"Support-vector machines are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis"* ~ WIKIPEDIA

Using the Support Vector Machine starting with only No_of_sentences and Text Length

```{r}
# Fitting SVM to the Training set and predicting the test set results
library(e1071)
SVM_classifier = svm(formula = Type ~ No_of_sentences + TextLength,
                 data = train_set,
                 type = 'C-classification',
                 kernel = 'linear')

# Predicting the Test set results
y_pred = predict(SVM_classifier,  newdata = test_set[2:3])

# Making the Confusion Matrix
require(caret)    
cm<-confusionMatrix(data=y_pred, 
                    reference=test_set$Type)
cm
Accuracy<-round(cm$overall[1],2)
Accuracy
```
The Accuracy is higher than all models of Logistic Regression

**MODEL 5 - SUPPORT VECTOR MACHINE**
In this final Algorithm I am going to use all the fields to predict 

```{r}
# Fitting SVM to the Training set and predicting the test set results
library(e1071)
SVM2_classifier = svm(formula = Type ~ .,
                     data = train_set,
                     type = 'C-classification',
                     kernel = 'linear')

# Predicting the Test set results
y_pred = predict(SVM2_classifier,  newdata = test_set[-1])

# Making the Confusion Matrix
require(caret)    
cm<-confusionMatrix(data=y_pred, 
                    reference=test_set$Type)
cm
Accuracy<-round(cm$overall[1],2)
Accuracy
```
Very happy with an accuracy of 84%


## CONCLUSION
Although the model yielded a great accuracy of 84%, This is a very simple dataset and I have only used simple measure of the text themselves. For future working I would add alot more data from different sources.
I also believe adding alternative categories such as:
Author, News Channel, Website, Topic, Country/ Region, can add a lot of value to a fake checking detection.

Thank you for taking the time to read my report



## REFERENCES

*https://datascienceplus.com/parsing-text-for-emotion-terms-analysis-visualization-using-r/#:~:text=R%20offers%20the%20get_nrc_sentiment%20function,sadness%2C%20joy%2C%20and%20disgust.*

*https://medium.com/swlh/exploring-sentiment-analysis-a6b53b026131*

*https://cran.r-project.org/web/packages/recosystem/vignettes/introduction.html*

*https://www.csie.ntu.edu.tw/~cjlin/papers/libmf/mf_adaptive_pakdd.pdf*



