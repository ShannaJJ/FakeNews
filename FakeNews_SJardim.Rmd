---
title: "Data Science: Capstone - Fake New Detection Project"
subtitle: "A lie gets halfway around the world before the truth has a chance to get its pants on - Winston Churchill"
author: "Shanna Jardim"
date: "01/04/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## INTRODUCTION

Misleading content such as fake news,  fake reviews, conspiracy theories and over exaggeration of the truth is nothing new. However it has increasingly become dangerous for online users in the last decade. What makes the present-day fake news most alarming is the speed with which it spreads.  According to a new survey by Pew Research Center, conducted in association with the John S. and James L. Knight Foundation - 62% of U.S. adults access news from social media.

Social Media is one of the biggest sources of spreading false news and poorly written news articles, which may have a certain degree of truth but are not entirely accurate or are completely made up. Usually, this kind of news is designed to promote or defend a certain agenda or biased opinion. One reason for the increase in misleading information is how easy it is for anyone to write fake reviews or articles on the web and there is no pre-approval process of the writings and spreading false information or beliefs across all social media. This poses a challenge to many readers to decide if the content is fake or true. This has given way to the development of Fact checking websites that use machine learning with natural language processing and research to detect fake news.

## GOAL OF PROJECT

The main challenge in this line of research is collecting quality data, i.e., instances of fake and real news articles on a balanced distribution of topics. In this assignment, I will use a dataset from Kaggle that has already separated a collection of articles into Fake or True. I will train an algorithm to detect if a new article is false or true by simply using metrics gathered from the text of the articles. 

#### Natural Language Processing

The news article data and headlines are in text format. Building automated machine learning models on text data involves cleaning and converting textual data to machine readable format. Natural-language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, and how to program computers to fruitfully process large amounts of natural language data.

## DATA SET

I am using the following datasets:

[**Source based Fake News Classification**](https://www.kaggle.com/ruchi798/source-based-news-classification)
[**Fake and real news dataset**](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset)

```{r loadpackages, echo=FALSE, warning=FALSE, message=FALSE}
#Load packages & import data
library(tidyverse) 
library(dplyr)
library(tidytext)
library(tibble)
library(ggplot2)
library(scales)
library(syuzhet)
library(tm)
library(ggplot2)


#FakeNews <- read_csv("Fake.csv")
#TrueNews <- read_csv("True.csv")
#News_Articles <- read_csv("news_articles.csv")



temp <- tempfile()
download.file("https://github.com/ShannaJJ/FakeNews/raw/main/Fake.zip",temp)
FakeNews <- read.csv(unz(temp, "Fake.csv"))
unlink(temp)
FakeNews <- as.data.frame(FakeNews) %>% mutate(title = as.character(title),
                                                text = as.character(text),
                                                date = as.character(date),
                                                subject = as.character(subject))

temp <- tempfile()
download.file("https://github.com/ShannaJJ/FakeNews/raw/main/True.zip",temp)
TrueNews <- read.csv(unz(temp, "True.csv"))
unlink(temp)
TrueNews <- as.data.frame(TrueNews) %>% mutate(title = as.character(title),
                                               text = as.character(text),
                                               date = as.character(date),
                                               subject = as.character(subject))

temp <- tempfile()
download.file("https://github.com/ShannaJJ/FakeNews/raw/main/news_articles.csv.zip",temp)
News_Articles <- read.csv(unz(temp, "news_articles.csv"))
unlink(temp)

News_Articles <- as.data.frame(News_Articles) %>% mutate(title = as.character(title_without_stopwords),
                                               text = as.character(text_without_stopwords),
                                               published = as.character(published),
                                               label = as.character(label))

```

## EXPLORING THE DATA
### Data Set 1
```{r }
as_tibble(head(FakeNews))
as_tibble(head(TrueNews))

nrow(FakeNews)
nrow(TrueNews)

```

For the purposes of this project I will only use the title and article text to identify whether an article is fake or true adding factors discussed in this document.

* **Observations**
+ there are more Fake news articles than true news
+ title and text can be joined into one Text field 
+ The author is unknown
+ The source of articles is unknown.
+ subject and date is not needed for this analysis so I will remove it.
+ Fake or True flag must be added when combining data

```{r, include=FALSE}
FakeNews <- subset(FakeNews, select = -c(date,subject) )
TrueNews <- subset(TrueNews, select = -c(date,subject) )
```

### Data Set 2
```{r message=FALSE}
as_tibble(head(News_Articles))
News_Articles %>% group_by(type) %>% summarize(NoArticles = n())
```

* **Observations**
+ Although "type" would be a great predicting factor, because Dataset 1 does not contain this information and for the purpose of removing any bias I will remove all other columns except title & text
+ Need to Concatenate the datasets 
+ (Will leave False & True Dataset separate for now)
+ Since we have more fake articles from the Dataset 1, I will only add the true articles from Dataset 2 and 
ensure there is an equal amount of Fake and True articles. This is to ensure there is no bias when predicting.

```{r, results="hide"}
News_Articles <- subset(News_Articles, select = c(title_without_stopwords,text_without_stopwords,label) )
News_Articles <- News_Articles %>% 
  rename(title = title_without_stopwords,
         text = text_without_stopwords,
         type = label)
```
```{r, echo=FALSE}
TrueNews2 <- filter(News_Articles, type == 'Real')
TrueNews2 <- subset(TrueNews2, select = -c(type) )
# Concatenate TrueNews2 with TrueNews
TrueNews <- rbind(TrueNews, TrueNews2)
FakeNews <- FakeNews[1:22218,]
```

```{r }
nrow(FakeNews)
nrow(TrueNews)
```

```{r, results="hide", error=FALSE }
# Removed unused dataframes
rm(TrueNews2, News_Articles)
```

## Word Analysis

1. **The following Text Preprocessing logic is used:** 
+ Add title & text to same field
+ Load the text as a corpus (A text corpus is a language resource consisting of a large and structured set of texts)
+ Convert the text to lower case
+ Remove numbers
+ Remove punctuation
+ Remove english common stopwords (Discussed later in this project)
+ Eliminate extra white spaces

```{r, echo=FALSE, warning=FALSE}
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("syuzhet")
library("ggplot2")
```

```{r, echo=FALSE}
# Add title & text to same field
FakeNews$text <- with(FakeNews, paste(title, text))
TrueNews$text <- with(TrueNews, paste(title, text))
# Load the data as a corpus

fake_corpus = VCorpus(VectorSource(FakeNews$text))
# Convert the text to lower case
fake_corpus = tm_map(fake_corpus, content_transformer(tolower))
# Remove numbers
fake_corpus = tm_map(fake_corpus, removeNumbers)
# Remove punctuation
fake_corpus = tm_map(fake_corpus, removePunctuation)
# Remove english common stopwords
fake_corpus = tm_map(fake_corpus, removeWords, stopwords())
# Eliminate extra white spaces
fake_corpus = tm_map(fake_corpus, stripWhitespace)
```

2. Text stemming - which reduces words to their root form. 
Stemming is changing the words into their original form and decreasing the number of word types or classes in the data. For example, the words “Running,” “Ran,” and “Runner” will be reduced to the word “run.
Observation: In my research I have noticed that this is not always the most accurate process as the stemming process sometime cuts off a full word.

Once the text preprocessig is not we can product a Word Cloud and table with the results
--------------------------------------------------------------------------
#### **Fake News Word Cloud**

```{r Fake News Word Cloud, echo=FALSE, fig.align = 'center'}
fake_corpus = tm_map(fake_corpus, stemDocument)

fake_dtm = DocumentTermMatrix(fake_corpus)
fake_dtm = removeSparseTerms(fake_dtm, 0.999)
fake_dataset = as.data.frame(as.matrix(fake_dtm))

#wordCloud
library(wordcloud)

fake_v = sort(colSums(fake_dataset),decreasing=TRUE)
myNames = names(fake_v)
fake_words = data.frame(word=myNames,freq=fake_v,type='fake')

wordcloud(words = fake_words$word, freq = fake_words$freq, min.freq = 5,
          max.words=100, random.order=FALSE, rot.per=0.40, 
          colors=brewer.pal(8, "Dark2"))
```

```{r results = 'asis', echo=FALSE}
# Display the top 5 most frequent words
knitr::kable(head(fake_words, 5), caption = "Fake News: Top5 most Frequent words")
```

--------------------------------------------------------------------------
**True News Word Cloud**

```{r True News Word Cloud, echo = FALSE, fig.align = 'center'}
true_corpus = VCorpus(VectorSource(TrueNews$text))
# Convert the text to lower case
true_corpus = tm_map(true_corpus, content_transformer(tolower))
# Remove numbers
true_corpus = tm_map(true_corpus, removeNumbers)
# Remove punctuation
true_corpus = tm_map(true_corpus, removePunctuation)
# Remove Stop words - They are the most commonly occurring words in a language and 
# have very little value in terms of gaining useful information. 
# They should be removed before performing further analysis.
true_corpus = tm_map(true_corpus, removeWords, stopwords())
# Eliminate extra white spaces
true_corpus = tm_map(true_corpus, stripWhitespace)
# Text stemming - which reduces words to their root form. 
true_corpus = tm_map(true_corpus, stemDocument)

true_dtm = DocumentTermMatrix(true_corpus)
true_dtm = removeSparseTerms(true_dtm, 0.999)
true_dataset = as.data.frame(as.matrix(true_dtm))

#wordCloud

true_v = sort(colSums(true_dataset),decreasing=TRUE)
myNames_true = names(true_v)
true_words = data.frame(word=myNames_true,freq=true_v,type='true')

wordcloud(words = true_words$word, freq = fake_words$freq, min.freq = 5,
          max.words=100, random.order=FALSE, rot.per=0.40, 
          colors=brewer.pal(8, "Dark2"))

Words <- rbind(fake_words, true_words)
```


```{r results = 'asis', echo=FALSE}
knitr::kable(head(true_words, 5), caption = "True News: Top5 most Frequent words")
```

## TRANSFORMATION OF THE DATA
```{r, echo = FALSE }
# A fake or truth indicator must be added as this is what we are predicting and join Fake & True Data to make a full data set.
FakeNews$Type <- c('Fake')
TrueNews$Type <- c('True')

NewsData <- rbind(FakeNews, TrueNews)
``` 
1. Check for any Null Values
```{r, results='hide', echo=FALSE}
#Check for any Null Values
anyNA(NewsData)

#True Check which columns contain Null
#colnames(NewsData)[colSums(is.na(NewsData)) > 0]
# Considering the text column is the main column we are using for analysis & prediction we will remove the NA values 
#NewsData <- na.omit(NewsData)

#nrow(NewsData)
#ncol(NewsData)

```

2. Add an ID to the dataset
```{r}
NewsData$ID <-seq_len(nrow(NewsData))
NewsData <- select(NewsData, ID, everything())
```
3. Remove Title as it was added to the text previously
```{r}
NewsData <- subset(NewsData, select = -c(title) )
```

```{r warning=FALSE, message=FALSE}
library(quanteda)
```
4. Add number of sentences per article
```{r}
NewsData$No_of_sentences <- nsentence(NewsData$text)
```
5. Add Number of characters per article
```{r}
NewsData$TextLength <- nchar(NewsData$text)
summary(NewsData$TextLength)
```
6. **Punctuation** 
Punctuation in natural language provides the grammatical context to the sentence.
Punctuations such as a commas, exclmation marks, full stop etc. wont add much value in understanding the
meaning of the sentence, however before removing all punctuation, questions and statements can be analysed. 

Calculating the number of Exclamation marks and question marks may add some interesting insight

Used the Sapply function to calculate number of exclamation marks
```{r}

NewsData$No_of_excl <- sapply(NewsData$text, 
                 function(x) length(unlist(strsplit(as.character(x), "\\!+"))))
```
Used the Sapply function to calculate number of question marks
```{r}
NewsData$No_of_question <- sapply(NewsData$text,
                    function(x) length(unlist(strsplit(as.character(x), "\\?+"))))
```

```{r, echo=FALSE, warning=FALSE}
##Count of exclamations & question marks in fake and true news avg in Fake and True
Punctuation <-NewsData %>% group_by(Type) %>% 
        summarise(Avg_Excl=round(mean(No_of_excl),2),
                  Avg_Ques=round(mean(No_of_question),2))

Punctuation <- Punctuation %>% gather("Punctuation", "Avg_per_Article", -Type)

ggplot(Punctuation, aes(x = Punctuation, y = Avg_per_Article, fill=Type)) +
  geom_col(position = "dodge") +   geom_text(aes(label=Avg_per_Article), position=position_dodge(width=0.9), vjust=-0.25)

NewsData$text<- gsub('[[:punct:]]', '', NewsData$text) #remove punctuation
```


There is a lot more punctuation used in false new than true, this may be directly linked to the amount of words, sentences or the length of the article. I will explore the correlation between these measures further down.


7. Make all text lower case
```{r}
NewsData$text <- tolower(NewsData$text) #make it lower case
```
8. Add word counts of top word found in Fake News
```{r}
NewsData$No_of_Wordtrump <- str_count(NewsData$text, "trump")
```
9. Add word counts of top word found in True News
```{r}
NewsData$No_of_Wordsaid <- str_count(NewsData$text, "said")
```

**View the all measures by type**
```{r echo=FALSE , message=FALSE}
Data_measures <- NewsData %>% group_by(Type) %>%
  summarize(Avg_no_Sentences = mean(No_of_sentences),
            Avg_TextLength = mean(TextLength),
            Max_TextLength = max(TextLength),
            Avg_no_excl = mean(No_of_excl),
            Avg_no_question = mean(No_of_question),
            Avg_no_trump = mean(No_of_Wordtrump),
            Avg_no_said = mean(No_of_Wordsaid))

```

```{r results = 'asis', echo=FALSE}
knitr::kable(Data_measures, caption = "Data Measures by article type")
```

**Correlations**
```{r echo=FALSE, message=FALSE}
NewsData  %>%  ggplot(aes(No_of_sentences, No_of_question, color=Type)) +
  geom_point() +
  geom_smooth() +
  scale_x_continuous(labels = scales::comma)+
  ggtitle("Correlation: No. of sentences and the No. of question marks per article")
```

```{r echo=FALSE}
NewsData  %>%  ggplot(aes(No_of_sentences, No_of_excl, color=Type)) +
  geom_point() +
  geom_smooth() +
  scale_x_continuous(labels = scales::comma)+
  ggtitle("Correlation: No. of sentences and the No. of exclamation marks per article")

```

The is an evident positive correlation between the number of sentences and the number of question marks however this is not so evident with the number of exclamations.

10. **Remove Stop words**
Stops Words (most common words in a language which do not provide much context) can be processed
and filtered from the text as they are more common and hold less useful information 
They should be removed before performing further analysis.
Some Stop words act like a connecting part of a sentences, for example, conjunctions “and”, “or” and “but”, prepositions like “of”, “in”, “from”, “to”, and the articles “a”, “an”, and “the”. 
Such stop words may take up valuable processing time, and hence removing stop words as a part of data 
preprocessing is a key first step in natural language processing.

```{r stopwords }
StopWords <- removeWords(NewsData$text, stopwords("en"))
StopWords <- data.frame(StopWords)
StopWords$ID <-seq_len(nrow(StopWords))
StopWords <- select(StopWords, ID, everything())
NewsData <- left_join(NewsData,StopWords,by="ID")
NewsData <- NewsData %>% rename(NoStop_text = StopWords)
```
After removing the stop words there are many white spaces left between words so I will remove the duplicate white spaces between the words.
```{r }
NewsData$NoStop_text <- str_replace_all(NewsData$NoStop_text, fixed("  "), " ")
```

11. Add number of words per article after removing Stop words
```{r }
NewsData$No_of_words <- sapply(strsplit(NewsData$NoStop_text, " "), length)
```

12. **Sentiment Analysis**
Sentiment Analysis: The study of extracted information to identify reactions, attitudes, context and emotions.
R offers the get_nrc_sentiment function via the Tidy or Syuzhet packages for analysis of emotion words expressed in text. Both packages implemented Saif Mohammad's NRC Emotion lexicon, comprised of several words for emotion expressions of anger, fear, anticipation, trust, surprise, sadness, joy, and disgust
```{r }
emotion <-get_nrc_sentiment(as.character(NewsData$NoStop_text))
```

```{r , results='hide', echo=FALSE}
#Take only ID and Fake Column and combine with emotion
IDFAke <- NewsData[c(1,3)]
#Take only the emotions - negative & postive will the used in actual News dataset
emotionDF <- cbind(NewsData[c(3)],emotion[c(1,2,3,4,5,6,7,8)])
emotionDF2 <- cbind(NewsData[c(3)],emotion[c(9,10)])

emotionGraph <- emotionDF %>% group_by(Type) %>%
  summarize_all(mean)
emotionGraph2 <- emotionDF2 %>% group_by(Type) %>%
  summarize_all(mean)

emotionGraph <- emotionGraph %>% gather("emotion", "Avg_No_of_Words", -Type)
emotionGraph2 <- emotionGraph2 %>% gather("emotion", "Avg_No_of_Words", -Type)
```

```{r , echo=FALSE}
#Create graph of Emotions Fake vs True Words
ggplot(emotionGraph, aes(x = emotion, y = Avg_No_of_Words, fill=Type)) +
  geom_col(position = "dodge")

```

**Observation**  
"Trust" is the only sentiment on average that is more in true news than in fake news.
I will add this observation to the working dataset

```{r , echo=FALSE}
ggplot(emotionGraph2, aes(x = emotion, y = Avg_No_of_Words, fill=Type)) +
  geom_col(position = "dodge")
```

I will use the negative, positive and trust as predicts for this analysis
```{r, echo=FALSE, message=FALSE }
#taking only negative , positive and trust column for the analysis
emotionNegPos<-emotion[c(8,9,10)]
emotionNegPos$ID <-seq_len(nrow(emotion))
emotionNegPos <- select(emotionNegPos, ID, everything())

NewsData<-left_join(NewsData,emotionNegPos)
```

**View the all additional measures by type**
```{r echo=FALSE, message=FALSE }
Data_measures <- NewsData %>% group_by(Type) %>%
  summarize(No_articles = n(),
            Avg_no_Sentences = mean(No_of_sentences),
            Avg_TextLength = mean(TextLength),
            Max_TextLength = max(TextLength),
            Avg_no_excl = mean(No_of_excl),
            Avg_no_question = mean(No_of_question),
            )

Data_measures2 <- NewsData %>% group_by(Type) %>%
  summarize(No_articles = n(),
            Avg_No_trump = mean(No_of_Wordtrump),
            Avg_No_said = mean(No_of_Wordsaid),
            Avg_No_trust = mean(trust),
            Avg_No_positive = mean(positive),
            Avg_No_negative = mean(negative))

```

```{r results = 'asis', echo=FALSE}
knitr::kable(Data_measures, caption = "Data Measures by article type")
knitr::kable(Data_measures2, caption = "Data Measures by article type")
```

```{r, echo=FALSE}
Dataset <- NewsData
NewsData <- subset(Dataset, select = -c(ID,text, NoStop_text))
```

## MODEL BUILDING & TRAINING

Now that we have prepared the data we can start building and training the data

Since most machine learning models only accept numerical variables, preprocessing the categorical variables becomes a necessary step. We need to convert these categorical variables to numbers such that the model is able to understand and extract valuable information. Here I will make Fake and True - 1 and 0 respectively 

Encoding categorical data
```{r Encoding categorical data, warning=FALSE}
NewsData$Type = factor(NewsData$Type,
                           levels= c('Fake','True'),
                           labels= c(1,0))
```

**Splitting the dataset into the Training set and Test set**
```{r , results='hide', message=FALSE, warning=FALSE}
library(caTools)
library(caret)
set.seed(123)
```

```{r Splitting the dataset, echo=FALSE, warning=FALSE}
split = sample.split(NewsData$Type, SplitRatio = 0.8)
train_set = subset(NewsData, split == TRUE)
test_set = subset(NewsData, split == FALSE)
```
**Feature scaling is neccessary before building the models**
Feature scaling is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization.
```{r , echo=FALSE}
train_set[,2:11] = scale(train_set[,2:11])
test_set[,2:11] = scale(test_set[,2:11])
```

### MODEL 1 - Logistic Regression

*"Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression)"* ~ WIKIPEDIA

For the first model, I am going to use the number of Sentences and Text Length only for prediction

```{r MODEL 1 - Logistic Regression}
# Fitting Logistic Regression to the Training set
classifier = glm(formula = Type ~ No_of_sentences + TextLength,
                 family = binomial,
                 data = train_set)
classifier

plot(classifier, 1)
```

Predicting the Test set results
```{r}
prob_pred = predict(classifier, type = 'response', newdata = test_set[2:3])
y_pred = ifelse(prob_pred > 0.5, 1, 0)
y_pred <-as.factor(y_pred)

```
A confusion matrix is a very useful tool for calibrating the output of a model and examining all possible outcomes of your predictions (true positive, true negative, false positive, false negative).
```{r, warning=FALSE}
# Making the Confusion Matrix
require(caret)    
cm<-confusionMatrix(data=y_pred, 
                    reference=test_set$Type)
```
```{r}
Accuracy<-cm$overall[1]
Accuracy
```

We can see from the graph that this is not a very accurate prediction with an outcome using only No. of Sentences and Text Length, lets see how this changes when adding more measures for prediction.


### MODEL 2 - Logistic Regression 2 : No of Words & Sentiment

Using Logistic Regression again with the sentiment of the words.
```{r MODEL 2 - Logistic Regression 2  , warning=FALSE }
# Fitting Logistic Regression to the Training set
classifier2 = glm(formula = Type ~ No_of_words + trust + negative + positive,
                 family = binomial,
                 data = train_set)
classifier2

# Predicting the Test set results
prob_pred2 = predict(classifier2, type = 'response', newdata = test_set[8:11])
y_pred2 = ifelse(prob_pred2 > 0.5, 1, 0)
y_pred2 <-as.factor(y_pred2)

# Making the Confusion Matrix
require(caret)    
cm<-confusionMatrix(data=y_pred2, 
                    reference=test_set$Type)
Accuracy<-cm$overall[1]
Accuracy
```

Result  is less than previous

### MODEL 3 - Logistic Regression 3
Using all measures
```{r MODEL 3 - Logistic Regression 3, warning=FALSE}
# Fitting Logistic Regression to the Training set
classifier3 = glm(formula = Type ~ .,
                  family = binomial,
                  data = train_set)
classifier3

# Predicting the Test set results
prob_pred3 = predict(classifier3, type = 'response', newdata = test_set[-1])

y_pred3 = ifelse(prob_pred3 > 0.5, 1, 0)
y_pred3 <-as.factor(y_pred3)

# Making the Confusion Matrix
require(caret)    
cm<-confusionMatrix(data=y_pred3, 
                    reference=test_set$Type)
Accuracy<-cm$overall[1]
Accuracy
```

Still not a very significant accuracy. Let's use a different method of classification prediction.

### MODEL 4 - SUPPORT VECTOR MACHINE
*"Support-vector machines are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis"* ~ WIKIPEDIA

Using the Support Vector Machine starting with only No_of_sentences and Text Length
```{r e1071, echo=FALSE, message=FALSE, error=FALSE}
library(e1071)
```
```{r MODEL 4 - SUPPORT VECTOR MACHINE , warning=FALSE}


# Fitting SVM to the Training set and predicting the test set results
SVM_classifier = svm(formula = Type ~ No_of_sentences + TextLength,
                 data = train_set,
                 type = 'C-classification',
                 kernel = 'linear')

# Predicting the Test set results
y_pred = predict(SVM_classifier,  newdata = test_set[2:3])

# Making the Confusion Matrix
require(caret)    
cm<-confusionMatrix(data=y_pred, 
                    reference=test_set$Type)
cm

Accuracy <-round(cm$overall[1],2)
Accuracy
```
An Accuracy for Support Vector Machine learning method is higher than all models of Logistic Regression

### MODEL 5 - SUPPORT VECTOR MACHINE
In this Algorithm I am going to use all the fields to predict if an article is True or Fake

```{r MODEL 5 - SUPPORT VECTOR MACHINE, message=FALSE, error=FALSE}
# Fitting SVM to the Training set and predicting the test set results
library(e1071)
SVM2_classifier = svm(formula = Type ~ .,
                     data = train_set,
                     type = 'C-classification',
                     kernel = 'linear')

# Predicting the Test set results
y_pred = predict(SVM2_classifier,  newdata = test_set[-1])

# Making the Confusion Matrix
require(caret)    
cm<-confusionMatrix(data=y_pred, 
                    reference=test_set$Type)
cm
Accuracy<-round(cm$overall[1],2)
Accuracy
```

This is a great deal higher. `Accuracy*100`% for Support Vector Machine learning method using all measures is higher than all models.


### MODEL 6 - DECISION TREE 
*"Decision tree is a graph to represent choices and their results in form of a tree. The nodes in the graph represent an event or choice and the edges of the graph represent the decision rules or conditions. It is mostly used in Machine Learning and Data Mining applications using R."* ~ TUTORIALSPOINT

I will rerun the dataset split into the Training set and Test set without Feature Scaling, in order to plot the Decision tree graph
```{r}
split = sample.split(NewsData$Type, SplitRatio = 0.8)
train_set = subset(NewsData, split == TRUE)
test_set = subset(NewsData, split == FALSE)
```

```{r, include=FALSE}
#install.packages('rpart')
library(rpart)
```

```{r MODEL 6 - DECISION TREE}

# Fitting SVM to the Training set and predicting the test set results
DT_classifier = rpart(formula = Type ~. , 
                   data = train_set)

# Predicting the Test set results
y_pred = predict(DT_classifier, newdata = test_set[-1],type = 'class')

# Making the Confusion Matrix
require(caret) 
cm<-confusionMatrix(data=y_pred, 
                    reference=test_set$Type)
cm

plot(DT_classifier)
text(DT_classifier)

Accuracy<-round(cm$overall[1],2)
Accuracy
```

Let us try one more classification Algorithm .

### MODEL 7 - RANDOM FOREST 

*Building further on the decision trees is the Random Forest. It is a powerful ensembling machine learning algorithm which works by creating multiple decision trees and then combining the output generated by each of the decision trees. * 

```{r, echo=FALSE}
library(caTools)
library(caret)
set.seed(123)

split = sample.split(NewsData$Type, SplitRatio = 0.8)
train_set = subset(NewsData, split == TRUE)
test_set = subset(NewsData, split == FALSE)
# Feature Scaling
train_set[,2:11] = scale(train_set[,2:11])
test_set[,2:11] = scale(test_set[,2:11])
```

```{r, echo=FALSE}
#install.packages('randomForest')
library(randomForest)
set.seed(123)
```

```{r MODEL 7 - RANDOM FOREST}
# Fitting SVM to the Training set and predicting the test set results
RF_classifier = randomForest(x = train_set[-1],
                          y = train_set$Type,
                          mtry = 6,
                          ntree = 500,
                          localImp = TRUE)
RF_classifier
```
By default, number of trees is 500 and number of variables tried at each split is 2 in this case. 
When I increased the mtry from 2 to 6, the error rate reduced from 8.25% to 8.17%. (No a huge difference)

```{r RANDOM FOREST predict}
# Predicting the Test set results
y_pred = predict(RF_classifier, newdata = test_set[-1])

# Making the Confusion Matrix
require(caret) 
cm<-confusionMatrix(data=y_pred, reference=test_set$Type)
```

```{r, echo=FALSE, message=FALSE}
#install.packages("randomForestExplainer")
library(randomForestExplainer)
```

```{r importance_frame , echo=FALSE}
importance_frame  <- measure_importance(RF_classifier)
save(importance_frame, file = "importance_frame.rda")
load("importance_frame.rda")
```

```{r results = 'asis', echo=FALSE}
knitr::kable(importance_frame, caption = "Importance Frame")
```

```{r plot_multi_way_importance}
plot_multi_way_importance(importance_frame, size_measure = "no_of_nodes")
```
Observe the marked negative relation between times_a_root and mean_min_depth. Also, the superiority of "No_of_question" & "No_of_excl" as well as "No_of_Wordsaid" is clear in all three dimensions plotted. 
The times_a_root would fall into the category of measuring the "relative power" of a variable compared to its "competitors". The times_a_root statistic measures the number of times a variable is "at the top" of a decision tree, i.e., how likely it is to be chosen first in the process of selecting split criteria. The no_of_node measures the number of times the variable is chosen at all as a splitting criterion among all of the sub sampled. In this situation and we can see that both of these variables have a comparable explanatory ability.

```{r}
Accuracy<-cm$overall[1]
Accuracy
```

The above comparison shows the true power of ensembling and the importance of using Random Forest over Decision Trees. Though Random Forest comes up with its own limitations -eg. number of factor levels a categorical variable can have - it still is one of the best models that can be used for classification. It is easy to use and tune as compared to some of the other complex models that I have not tried in this report. Random Forest still provides a good level of accuracy.. 



## CONCLUSION
Although the model yielded a pretty good accuracy of `Accuracy`%, This is a limited sample dataset.  I have only used a set amount of measures for the text. To further this study and analysis, it would be beneficial to gather data from different sources and on various topics as well as adding alternative dimensions such as:
Author, News Channel, Website, Topic, Country or Region, etc.
This can add a lot more value to a fake checking detection.
Word Associations, Term Frequencies and phases as well as stemming can also be used to further the analysis on fake news.



Thank you for taking the time to read this report.






## REFERENCES

*https://datascienceplus.com/parsing-text-for-emotion-terms-analysis-visualization-using-r/*
*#:~:text=R%20offers%20the%20get_nrc_sentiment%20function,sadness%2C%20joy%2C%20and%20disgust.*

*https://medium.com/swlh/exploring-sentiment-analysis-a6b53b026131*

*https://cran.r-project.org/web/packages/recosystem/vignettes/introduction.html*

*https://www.csie.ntu.edu.tw/~cjlin/papers/libmf/mf_adaptive_pakdd.pdf*

*https://scholar.smu.edu/cgi/viewcontent.cgi?article=1036&context=datasciencereview*

*https://cran.r-project.org/web/packages/randomForestExplainer/vignettes/randomForestExplainer.html*

*https://www.tutorialspoint.com/r/r_decision_tree.htm#:~:text=Decision%20tree%20is%20a%20graph,*
*Data%20Mining%20applications%20using%20R.*

